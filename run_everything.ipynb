{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.4 64-bit",
   "display_name": "Python 3.7.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### This notebook serves the purpose of running everithing in one go - train, save the model where you want, resume training, see the predictions etc.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_preprocess_input import *\n",
    "from models import *\n",
    "from base_train import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/sme-freecorpus.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# clean very special char\n",
    "text = text.replace(\"¶\", \"\").replace('•', '').replace('□', '').replace('§', '').replace('\\uf03d', '').replace('π', '').replace('●', '').replace('µ', '').replace('º', '').replace('文', '').replace('中', '').replace('⅞', '').replace('½', '').replace('⅓', '').replace('¾', '').replace('¹', '').replace('³', '').replace('\\t', '')\n",
    "# remove numbers\n",
    "text = re.sub(r'[0-9]+', '', text)\n",
    "# remove russian text (it is in data)\n",
    "text = re.sub(r\"[А-Яа-я]\", '', text) \n",
    "# remove puctuation\n",
    "text = re.sub(r\"[^\\w\\s]\", \"\", text) \n",
    "\n",
    "# encode the text \n",
    "# 1. int2char, integers to characters\n",
    "# 2. char2int, characters to unique integers\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LSTM(\n  (lstm): LSTM(224, 512, num_layers=3, batch_first=True, dropout=0.5)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc): Linear(in_features=512, out_features=224, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "n_hidden=512\n",
    "n_layers=3\n",
    "# default values \n",
    "drop_prob = 0.5\n",
    "lr=1\n",
    "bidirectional = False\n",
    "\n",
    "# load one of the models from models.py\n",
    "model = LSTM(chars, device, bidirectional, n_hidden, n_layers)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Resuming lstm3_epoch.pt from epoch 3 ...\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "expected device cuda:0 and dtype Float but got device cpu and dtype Float",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-953f5cae0fc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lstm3_epoch.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_from_saved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbidirectional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbidirectional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/SAMI_ML/base_train.py\u001b[0m in \u001b[0;36mtrain_and_save\u001b[0;34m(model, data, device, model_name, epochs, batch_size, seq_length, lr, clip, val_frac, resume_from_saved, bidirectional)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0;31m# put back to cuda if was detached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected device cuda:0 and dtype Float but got device cpu and dtype Float"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 300\n",
    "n_epochs = 7 # small because for testing\n",
    "\n",
    "# train the model\n",
    "train_and_save(model, encoded, device, model_name='lstm3_epoch.pt', epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.0001, resume_from_saved=True, bidirectional=bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'jagĺĺâÏʹrÏn  \\nMaŋŋil leat     jahkái  doallu mearriduvvui  ja  máhtto     ru ja sámi álbmoga mearkkašumit \\nDán lea servodat gielddaid geavaheapmi leat suohkaniin  \\n \\n                              lea         ru   \\n   \\nVuollel   rájes ja ovddidan ovttadássásaš  bisuheapmi mielde govddidit mearrasámi golbmalaš doaibma \\n   ja go daid bisuhuvvo siiddus  máná veahkki  ja   doarjja   ru \\nSámedikki birra  \\n   Duodjeráhkadit dasa  mánáidgárddiige sámi giellaolu                                                                       \\n        \\n                                                                                           \\nVálljejit sámegiela   \\n                         \\nSii                                                                \\nSámedikki birra  Sámi kultuvrras \\n   Dan  sámegiella                             \\n                                                                  \\nSápmelaččat ja dat loguid guoská sámi kulturmuittoid buore doaimmain ja dahkamat ja servodaga buohkaid juohke go dan guovlun \\nSámedikki galgá dain lea oahpahusat leat sihkkarastit ovttasbarggu dan dahkkojuvvon ja dat deaŧalaš doarjaga bargojuvvo gielddaiguin \\n Sámedikki dássádusat galgá dárkkisteapmái ja sámegiela vuoigatvuođaid ja galgá deaŧalaš dásis \\n \\nSámedikki birra  \\n \\nSámi oahppaneavvuid ja duhtadeamit doarjagat muhto máhttodiehtun ja guovlluin geat leat dat govva lea sámi goluhuvvon olbmuide gaskkastan ja dan dárbbašuvvojit dásseárvosaš ja sierra dihte oahppi go birasgusket ovdamearkan golbma sámi mielas leat deaŧalaš sámi guovllus ja ovdáneamis \\n  \\n Sámegillii \\nDuođašteapmái  Sámediggi oaivvilduvvo sámi duodjeoahpaheapmi ja birra guovddáža ja oaivilii ja doaibmabidju    \\nSámediggi doarjaga dieđuid buoret doarjja sámi museat mátkkáštallamii Duođaštit oasseválddit ja duođašteami maid ollu dollojuvvon juohke dáidda ja dutkanprošeavtta meannudeami maid sámi guovllus ja oahpponeavvuid mielde dahje ovdánahttin ovttajienalaččat man daid dearvvašvuođabárkka dihte sámi kultuvrralaš dárbbut ja máŋgga dásis ja divttus \\nDoarjja lea oahpahusas mat dutkandoaibmabijuid doarjjaortnega mielde jahkásaččat ja doaibmabijuid ja suodjaleami doaibmabijuid ja ovddideamis lea min bálvalusat \\nDoarjjastivra dan geavahan ja dárbbašlaš dárbu muhto dál doaibmabijuiguin go dán oahppi man guovlluin \\nSámediggi oaivvilda ahte guhtet oassálastin mielde dasa giellagiela ovttastahttin ja sáhttá ollu dán ovddasvástádus ja ovdáneamis ja ovdamearkkat dahje dat leat oktavuohta ja guovlluin leat das lea stuvra sámi guovllus galgá ovddidit sámegillii ja duođaštuvvot dási buori ovttasbargošiehtadusa ja dárbbašlaš deavvu go sámi kulturmundin ja doarjjaoažžu geat ovdánahttit dutkan \\nOahpahusa mearkašeami mii lea sámi mielas lea vuođđooahpahusa birra \\nSámediggi lea vuollái mánáidgárdefierpmádaga stuorát stuorát maid gullet máhttot ja dárbbut dan geavahit doaimmaide \\nSámi olbmiide leat sámi mánáidgárdefálaldaga mii guoská oažžut siskkobealde galgá ollislaš váldojuvvon ovddideapmái ja barggu deaivvadeapmi miel'"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "from predict import *\n",
    "\n",
    "model, _ , _, _, _, _  = load_checkpoint(\"lstm3_epoch.pt\", model, opt)\n",
    "\n",
    "show_sample(model, 2000, device, prime='ja', top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}